# -------------------- Gradient Descent Training ----------------
#
# Training for a linear feed forward network using gradient descent.
# The training is controlled by two parameters at each stage
#
# epsilon - the proportion of the weight derivatives used to correct
# alpha   - the proportion of the previous correction to use (the momentum
# 

loaddefs "nn_maths;
loaddefs "nn_feed_forward_net;

write 'nn_gd_training loaded';




# gd_feed_back
# ------------
# Adjust the network using the expected output and the data from a
# feed forward pass. We adjust the weights at each layer to move 
# towards a minimum of the mean square error (MSE) of the final output
# vs the expected. This is done using the partial derivatives of 
# the MSE wrt the weights. 
#

gd_feed_back is transformer trans_fn trans_df operation epsilon alpha temp expected lwc {
	netw := [];
	momentum := lwc;
	% Start with output layer;
	layer_outputs layer_inputs activation layer_weights := first temp;
	errors := layer_outputs - expected;
	% Compute and print the MSE;
	mse := (sum (errors*errors))/2.0;
	% write 'MSE' mse;
	% Compute the derivates of the transfer function for the activations;
	% In this version for sigmoid and tanh we compute from the outputs;
	activation_derivs := trans_df [activation, layer_outputs];
	deltas := errors * activation_derivs;
	weight_derivs := deltas eachleft * (layer_inputs link [1.0]);
	weight_correction := epsilon*weight_derivs;
	if momentum then
		weight_correction := weight_correction eachboth + (alpha*momentum);
	endif;
	momentum := weight_correction;
	% Compute the new weights;
	new_weights := layer_weights eachboth + weight_correction;
	netw := new_weights hitch netw;
	% Compute the partial derivatives for the previous layer;
	partial_derivs := each sum (deltas eachboth * layer_weights);
	% Move on to inner layers;
	for layer with rest temp do
		layer_outputs layer_inputs activation layer_weights := layer;
		activation_derivs := trans_df [activation, layer_outputs];
		deltas := partial_derivs * activation_derivs;
		weight_derivs := deltas eachleft * (layer_inputs link [1.0]);
		weight_correction := epsilon*weight_derivs;
		weight_correction := weight_correction eachboth + (alpha*momentum);
		momentum := weight_correction;
		% Compute the new weights;
		new_weights := layer_weights eachboth + weight_correction;
		netw := new_weights hitch netw;
		partial_derivs := each sum (deltas eachboth * layer_weights);
	endfor;
	[netw, momentum, mse]}



# gd_online_train_net
# -------------------
# Train a network using a set of data consisting of inputs and 
# expected outputs. Perform a number of passes to continuously
# improve the weights.

gd_online_train_net is transformer trans_fn trans_df operation nnet tdata t_params t_control {
  epsilon alpha := t_params;
  n_iter lmse := t_control; 
  write 'gd_online_train_net' epsilon alpha lmse;
  netw := nnet;
  history := [];
  % for the initial momentum;
  momentum := [];
  i := 0;
  repeat
    mse := 0.0;
    for d with tdata do
      t_input expected_output := d;
      temp := nn_feed_net trans_fn netw t_input;
      netw momentum mse2 := gd_feed_back [trans_fn, trans_df] epsilon alpha temp expected_output momentum;
      mse := mse + mse2;
    endfor;
    i := i + 1;
    history := mse hitch history;
    %write 'Final mse' mse lmse;
  until (mse < lmse) or (i > n_iter)
  endrepeat;
  [netw, history]}




# =============== Some Useful Definitions =================================

gd_online_sigmoid_train_net is (gd_online_train_net [sigmoid_fn, sigmoid_df]);
gd_online_tanh_train_net is (gd_online_train_net [tanh_fn, tanh_df]);


